<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>编程赛算法篇</title>
    <url>/2020/04/03/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>　　此篇章用于写算法笔记，题目刷起来！</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习开篇</title>
    <url>/2020/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>　　今天是2020年3月29号，最近开始跟着吴恩达老师入门机器学习，以后就在这里记录一下自己学习历程，<br>写出来的内容完全只是个人收获或感悟，所以肯定有不少错误存在，单纯记录下。<br>　　好了，就这样吧，加油！</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(1)三种梯度下降法</title>
    <url>/2020/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-BGD%E3%80%81SGD%E3%80%81MBGD/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>######　　线性回归问题中，我们可以假设线性回归方程：<br>####　　　　　　　　　　hθ(X)=θ0+θ1x1+θ2x2+···+θnxn。<br>######（其中xi代表系数，θi为各特征向量的系数，此处认为x0=1，故不写出，因此<br>######有了常数项），但实际上特征与结果hθ关系不一定是这个，所以接下来我们要<br>######引入亏损函数的概念，常用的亏损函数为均值方差：<br>####　　　　　　　　　　J(θ0,θ1,θ2·····θn)=∑i=1m(hθ(xi)−yi)^2<br>######从公式上就可以看出来,亏损方程与θi相关，为了使回归方程尽可能的表现出各<br>######特征与h(θ)的关系，我们需要调整各个θi，但如何调整能达到目的呢？我们回<br>######到亏损函数上来，若要达到目的，我们就要使得亏损函数尽可能的小，所以下<br>######一步操作就是最小化亏损函数。<br>######最小化亏损函数，使用梯度下降法无疑是个好方法。WHY?那是因为刚入门的<br>######我只会这一种方法<em>_</em>~~<br>######　　不废话，直接上解释，利用单个θ讲解，单个或者多特征的线性回归只要<br>######扩(zi)展(xue)下(qu):<br>#<img src="/.io//%E6%A2%AF%E5%BA%A6.png" alt><br>######　　整个过程的目的是找到合适的θ值让亏损函数最小化，现在我们先假设θ<br>######等于A，在目标值θ=B的左边，我们要是慢慢让θ想B靠拢，如何做到？我们试<br>######着在A处画切线，切线的斜率就是J(θ)在这一点的导数，不知道这个的可以联系<br>######我，我这边工地正好缺些人手。J(θ)在这一点的导数为负数，那θ减去导数的值<br>######不就慢慢向B靠拢了吗。同理，假设θ初始在C点的话，操作也是如此。<br>#<img src="/.io//%E6%A0%B8%E5%BF%83%E5%85%AC%E5%BC%8F.png" alt="核心公式"><br>######　　公式里要注意的一个是α的值，这个也是我们在一开始要设置的，但设置它<br>######的值要注意，只不能太大，也不要太小。太大的话在计算的时候新的θ会在B点<br>######两边来回震荡，可能始终都到不了B点，太小了也会收敛太慢，每一次前进一点<br>######点，要走到何时才能走到B点，走到我脱单都估计，诶，算了，肯定在我脱单之<br>######前走到。<br>#####—————–假—装—我—是—一—条—分—割—线————-<br>###三种梯度下降算法比较：<br>　<br>###【BGD–批量梯度下降】<br>######　　在每一次进行梯度下降更新theta值时，会采用所有样本数据进行计<br>######算损失函数求偏导，从而更新theta值。其优点就是提高theta值的准确性<br>######，因采用所有样本数据计算，故缺点就是计算复杂程度高，时间消耗大。<br>######—————————————————————–</p>
<p>###【SGD–随机梯度下降】<br>######　　在每一次进行梯度下降更新theta值时，只使用单个样本进行计算<br>######优点就是提高t训练速度，相反的这种方式就没有BGD得到的结果的准确<br>######性高。<br>######—————————————————————–</p>
<p>###【MBGD–小批量梯度下降】<br>######　　这种方法算是为了同时考虑到速度和准确性在。每一次进行梯度下降<br>######更新theta值时，使用部分样本数据进行计算<br>######性高。<br>######—————————————————————–<br>###课后作业·劝退就得靠实战演练<br>#####题目：根据城市人口数量，预测开小吃店的利润 数据在ex1data1.txt里，第<br>#####一列是城市人口数量，第二列是该城市小吃店利润。<br>#####我的答案：(注：我的答案和题目来源的作者有偏差，我不知道哪儿错了)<br>#####另:文章结尾附上题目来源链接【白嫖真香,下次一定警告,老铁点个赞&lt;_&lt;】<br>#<img src="/.io//%E6%A0%B8%E5%BF%83%E5%85%AC%E5%BC%8F.png" alt="核心公式"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(X,y,theta)</span>:</span></span><br><span class="line">    cost_mar = np.power(((X*theta.T)-y),<span class="number">2</span>)</span><br><span class="line">    cost_sum = np.sum(cost_mar)/(<span class="number">2</span>*len(X))</span><br><span class="line">    <span class="keyword">return</span> cost_sum</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(X,y,theta,aph,time)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(time):</span><br><span class="line">        temp1 = theta</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line">            temp = X[:,j]</span><br><span class="line">            theta[<span class="number">0</span>,j]=theta[<span class="number">0</span>,j]-aph/len(X)*(np.sum((X*temp1.T-y).T*temp))</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">path = <span class="string">'C:/Users/Lenovo/Desktop/MLdata/ex1/ex1data1.txt'</span></span><br><span class="line">data = pd.read_csv(path,header=<span class="literal">None</span>,names=[<span class="string">'population'</span>,<span class="string">'profit'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data.insert(<span class="number">0</span>,<span class="string">'ones'</span>,<span class="number">1</span>)</span><br><span class="line">col = data.shape[<span class="number">1</span>]</span><br><span class="line">X = data.iloc[:,:<span class="number">-1</span>]</span><br><span class="line">y = data.iloc[:,col<span class="number">-1</span>:col]</span><br><span class="line">theta = np.array([[<span class="number">0.0</span>,<span class="number">0.0</span>]])</span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix(theta)</span><br><span class="line">aph = <span class="number">0.003</span></span><br><span class="line">time = <span class="number">10000</span></span><br><span class="line"><span class="comment">#print(X)</span></span><br><span class="line">theta = gradient(X,y,theta,aph,time)</span><br><span class="line">print(<span class="string">'theta:'</span>,theta)<span class="comment">#训练结果得到的theta向量</span></span><br><span class="line">predict1 = [<span class="number">1</span>,<span class="number">3.5</span>]*(theta).T</span><br><span class="line">print(<span class="string">'predict1:'</span>,predict1)</span><br><span class="line">predict2 = [<span class="number">1</span>,<span class="number">7</span>]*(theta).T</span><br><span class="line">print(<span class="string">'predict2:'</span>,predict2)</span><br><span class="line"></span><br><span class="line">x = np.linspace(data.population.min(), data.population.max(), <span class="number">100</span>)</span><br><span class="line">f = theta[<span class="number">0</span>, <span class="number">0</span>] + (theta[<span class="number">0</span>, <span class="number">1</span>] * x)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">ax.scatter(data.population, data.profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">ax.legend(loc=<span class="number">2</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Predicted Profit for Population Size'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>###训练结果(采用BGD下降算法)<br>######这里我将theta初始值假设为[0,0],aph为0.03，迭代更新次数为10000<br><img src="/.io//result.png" alt><br><img src="/.io//chart1.png" alt><br>#####—————–假—装—我—是—一—条—分—割—线————-<br>#####　　老铁们，看到错误一定要在评论里告诉我，不告诉我的都是我搬砖路上<br>#####的绊脚石。最后附上题目资源链接。谢谢大家!相互学习!共同进步!一起搬砖!<br>####下面给出让人血脉喷张,内容不可描述的网址,一定要自己晚上偷偷看哦,滚吧【狗头】：###<a href="https://www.kesci.com/home/project/5da16a37037db3002d441810" target="_blank" rel="noopener">https://www.kesci.com/home/project/5da16a37037db3002d441810</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>About  Me</title>
    <url>/2020/03/27/About/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>姓名：张玉想<br>性别：男<br>现居地：江西南昌<br>职业：东华理工大学 | 计算机科学与技术 | 本科<br>爱好：人工智能 | 计算机视觉 | 摄影 | 旅行<br>github: <a href="https://github.com/zyx-cv" target="_blank" rel="noopener">https://github.com/zyx-cv</a><br>QQ：1300281401(非诚勿扰)<br>哔哩哔哩：Shutterbug张<br>图虫：Shutterbug张<br>CSDN：小张小张,考试不慌</p>
]]></content>
  </entry>
</search>
