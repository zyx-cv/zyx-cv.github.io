<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>编程赛算法篇</title>
    <url>/2020/04/03/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>　　此篇章用于写算法笔记，题目刷起来！</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习开篇</title>
    <url>/2020/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>　　今天是2020年3月29号，最近开始跟着吴恩达老师入门机器学习，以后就在这里记录一下自己学习历程，<br>写出来的内容完全只是个人收获或感悟，所以肯定有不少错误存在，单纯记录下。<br>　　好了，就这样吧，加油！</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(1)三种梯度下降法</title>
    <url>/2020/03/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-BGD%E3%80%81SGD%E3%80%81MBGD/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="线性回归问题中，我们可以假设线性回归方程："><a href="#线性回归问题中，我们可以假设线性回归方程：" class="headerlink" title="线性回归问题中，我们可以假设线性回归方程："></a>线性回归问题中，我们可以假设线性回归方程：</h3><h3 id="hθ-X-θ0-θ1x1-θ2x2-···-θnxn。"><a href="#hθ-X-θ0-θ1x1-θ2x2-···-θnxn。" class="headerlink" title="　　　　　　　　　　hθ(X)=θ0+θ1x1+θ2x2+···+θnxn。"></a>　　　　　　　　　　hθ(X)=θ0+θ1x1+θ2x2+···+θnxn。</h3><h3 id="（其中xi代表系数，θi为各特征向量的系数，此处认为x0-1-，故不写出，因此有了常数项"><a href="#（其中xi代表系数，θi为各特征向量的系数，此处认为x0-1-，故不写出，因此有了常数项" class="headerlink" title="（其中xi代表系数，θi为各特征向量的系数，此处认为x0=1######，故不写出，因此有了常数项"></a>（其中xi代表系数，θi为各特征向量的系数，此处认为x0=1######，故不写出，因此有了常数项</h3><h3 id="），但实际上特征与结果hθ关系不一定是这个，所以接下来我们要"><a href="#），但实际上特征与结果hθ关系不一定是这个，所以接下来我们要" class="headerlink" title="），但实际上特征与结果hθ关系不一定是这个，所以接下来我们要"></a>），但实际上特征与结果hθ关系不一定是这个，所以接下来我们要</h3><h3 id="引入亏损函数的概念，常用的亏损函数为均值方差："><a href="#引入亏损函数的概念，常用的亏损函数为均值方差：" class="headerlink" title="引入亏损函数的概念，常用的亏损函数为均值方差："></a>引入亏损函数的概念，常用的亏损函数为均值方差：</h3><h3 id="J-θ0-θ1-θ2·····θn-∑i-1m-hθ-xi-−yi-2"><a href="#J-θ0-θ1-θ2·····θn-∑i-1m-hθ-xi-−yi-2" class="headerlink" title="　　　　　　　　　　J(θ0,θ1,θ2·····θn)=∑i=1m(hθ(xi)−yi)^2"></a>　　　　　　　　　　J(θ0,θ1,θ2·····θn)=∑i=1m(hθ(xi)−yi)^2</h3><h3 id="从公式上就可以看出来-亏损方程与θi相关，为了使回归方程尽可能的表现出各特征与h-θ-的关系，"><a href="#从公式上就可以看出来-亏损方程与θi相关，为了使回归方程尽可能的表现出各特征与h-θ-的关系，" class="headerlink" title="从公式上就可以看出来,亏损方程与θi相关，为了使回归方程尽可能的表现出各特征与h(θ)的关系，"></a>从公式上就可以看出来,亏损方程与θi相关，为了使回归方程尽可能的表现出各特征与h(θ)的关系，</h3><h3 id="我们需要调整各个θi，但如何调整能达到目的呢？我们回"><a href="#我们需要调整各个θi，但如何调整能达到目的呢？我们回" class="headerlink" title="我们需要调整各个θi，但如何调整能达到目的呢？我们回"></a>我们需要调整各个θi，但如何调整能达到目的呢？我们回</h3><h3 id="到亏损函数上来，若要达到目的，我们就要使得亏损函数尽可能的小，所以下-一步操作就是最小化"><a href="#到亏损函数上来，若要达到目的，我们就要使得亏损函数尽可能的小，所以下-一步操作就是最小化" class="headerlink" title="到亏损函数上来，若要达到目的，我们就要使得亏损函数尽可能的小，所以下 一步操作就是最小化"></a>到亏损函数上来，若要达到目的，我们就要使得亏损函数尽可能的小，所以下 一步操作就是最小化</h3><h3 id="亏损函数。"><a href="#亏损函数。" class="headerlink" title="亏损函数。"></a>亏损函数。</h3><h3 id="最小化亏损函数，使用梯度下降法无疑是个好方法。WHY-那是因为刚入门的我只会这一种方法"><a href="#最小化亏损函数，使用梯度下降法无疑是个好方法。WHY-那是因为刚入门的我只会这一种方法" class="headerlink" title="最小化亏损函数，使用梯度下降法无疑是个好方法。WHY?那是因为刚入门的我只会这一种方法~~"></a>最小化亏损函数，使用梯度下降法无疑是个好方法。WHY?那是因为刚入门的我只会这一种方法~~</h3><h3 id="不废话，直接上解释，利用单个θ讲解，单个或者多特征的线性回归只要扩-zi-展-xue-下-qu"><a href="#不废话，直接上解释，利用单个θ讲解，单个或者多特征的线性回归只要扩-zi-展-xue-下-qu" class="headerlink" title="　　不废话，直接上解释，利用单个θ讲解，单个或者多特征的线性回归只要扩(zi)展(xue)下(qu):"></a>　　不废话，直接上解释，利用单个θ讲解，单个或者多特征的线性回归只要扩(zi)展(xue)下(qu):</h3><p><img src="/.io//%E6%A2%AF%E5%BA%A6.png" alt></p>
<h3 id="整个过程的目的是找到合适的θ值让亏损函数最小化，现在我们先假设θ等于A，在目标值θ-B"><a href="#整个过程的目的是找到合适的θ值让亏损函数最小化，现在我们先假设θ等于A，在目标值θ-B" class="headerlink" title="　　整个过程的目的是找到合适的θ值让亏损函数最小化，现在我们先假设θ等于A，在目标值θ=B"></a>　　整个过程的目的是找到合适的θ值让亏损函数最小化，现在我们先假设θ等于A，在目标值θ=B</h3><h3 id="的左边，我们要是慢慢让θ想B靠拢，如何做到？我们试着在A处画切线，切线的斜率就是J-θ-在这"><a href="#的左边，我们要是慢慢让θ想B靠拢，如何做到？我们试着在A处画切线，切线的斜率就是J-θ-在这" class="headerlink" title="的左边，我们要是慢慢让θ想B靠拢，如何做到？我们试着在A处画切线，切线的斜率就是J(θ)在这"></a>的左边，我们要是慢慢让θ想B靠拢，如何做到？我们试着在A处画切线，切线的斜率就是J(θ)在这</h3><h3 id="一点的导数，不知道这个的可以联系我，我这边工地正好缺些人手。J-θ-在这一点的导数为负数，那θ减"><a href="#一点的导数，不知道这个的可以联系我，我这边工地正好缺些人手。J-θ-在这一点的导数为负数，那θ减" class="headerlink" title="一点的导数，不知道这个的可以联系我，我这边工地正好缺些人手。J(θ)在这一点的导数为负数，那θ减"></a>一点的导数，不知道这个的可以联系我，我这边工地正好缺些人手。J(θ)在这一点的导数为负数，那θ减</h3><h3 id="去导数的值不就慢慢向B靠拢了吗。同理，假设θ初始在C点的话，操作也是如此。"><a href="#去导数的值不就慢慢向B靠拢了吗。同理，假设θ初始在C点的话，操作也是如此。" class="headerlink" title="去导数的值不就慢慢向B靠拢了吗。同理，假设θ初始在C点的话，操作也是如此。"></a>去导数的值不就慢慢向B靠拢了吗。同理，假设θ初始在C点的话，操作也是如此。</h3><p><img src="/.io//%E6%A0%B8%E5%BF%83%E5%85%AC%E5%BC%8F.png" alt></p>
<h3 id="公式里要注意的一个是α的值，这个也是我们在一开始要设置的，但设置它-的值要注意，只不"><a href="#公式里要注意的一个是α的值，这个也是我们在一开始要设置的，但设置它-的值要注意，只不" class="headerlink" title="　　公式里要注意的一个是α的值，这个也是我们在一开始要设置的，但设置它 的值要注意，只不"></a>　　公式里要注意的一个是α的值，这个也是我们在一开始要设置的，但设置它 的值要注意，只不</h3><h3 id="能太大，也不要太小。太大的话在计算的时候新的θ会在B点"><a href="#能太大，也不要太小。太大的话在计算的时候新的θ会在B点" class="headerlink" title="能太大，也不要太小。太大的话在计算的时候新的θ会在B点"></a>能太大，也不要太小。太大的话在计算的时候新的θ会在B点</h3><h3 id="两边来回震荡，可能始终都到不了B点，太小了也会收敛太慢，每一次前进一点点，要走到何时才能"><a href="#两边来回震荡，可能始终都到不了B点，太小了也会收敛太慢，每一次前进一点点，要走到何时才能" class="headerlink" title="两边来回震荡，可能始终都到不了B点，太小了也会收敛太慢，每一次前进一点点，要走到何时才能"></a>两边来回震荡，可能始终都到不了B点，太小了也会收敛太慢，每一次前进一点点，要走到何时才能</h3><h3 id="走到B点，走到我脱单都估计，诶，算了，肯定在我脱单之"><a href="#走到B点，走到我脱单都估计，诶，算了，肯定在我脱单之" class="headerlink" title="走到B点，走到我脱单都估计，诶，算了，肯定在我脱单之"></a>走到B点，走到我脱单都估计，诶，算了，肯定在我脱单之</h3><h3 id="前走到。"><a href="#前走到。" class="headerlink" title="前走到。"></a>前走到。</h3><h4 id="———假——-装——-我——-是——-一——-条——-分——-割——-线——–"><a href="#———假——-装——-我——-是——-一——-条——-分——-割——-线——–" class="headerlink" title="———假——-装——-我——-是——-一——-条——-分——-割——-线——–"></a>———假——-装——-我——-是——-一——-条——-分——-割——-线——–</h4><h1 id="三种梯度下降算法比较："><a href="#三种梯度下降算法比较：" class="headerlink" title="三种梯度下降算法比较：　"></a>三种梯度下降算法比较：　</h1><h2 id="【BGD–批量梯度下降】"><a href="#【BGD–批量梯度下降】" class="headerlink" title="【BGD–批量梯度下降】"></a>【BGD–批量梯度下降】</h2><h3 id="在每一次进行梯度下降更新theta值时，会采用所有样本数据进行计算损失函数求偏导，从而更"><a href="#在每一次进行梯度下降更新theta值时，会采用所有样本数据进行计算损失函数求偏导，从而更" class="headerlink" title="　　在每一次进行梯度下降更新theta值时，会采用所有样本数据进行计算损失函数求偏导，从而更"></a>　　在每一次进行梯度下降更新theta值时，会采用所有样本数据进行计算损失函数求偏导，从而更</h3><h3 id="新theta值。其优点就是提高theta值的准确性，因采用所有样"><a href="#新theta值。其优点就是提高theta值的准确性，因采用所有样" class="headerlink" title="新theta值。其优点就是提高theta值的准确性，因采用所有样"></a>新theta值。其优点就是提高theta值的准确性，因采用所有样</h3><h6 id="本数据计算，故缺点就是计算复杂程度高，时间消耗大。"><a href="#本数据计算，故缺点就是计算复杂程度高，时间消耗大。" class="headerlink" title="本数据计算，故缺点就是计算复杂程度高，时间消耗大。"></a>本数据计算，故缺点就是计算复杂程度高，时间消耗大。</h6><h4 id="————————————————————————————————————————————————————"><a href="#————————————————————————————————————————————————————" class="headerlink" title="————————————————————————————————————————————————————-"></a>————————————————————————————————————————————————————-</h4><h2 id="【SGD–随机梯度下降】"><a href="#【SGD–随机梯度下降】" class="headerlink" title="【SGD–随机梯度下降】"></a>【SGD–随机梯度下降】</h2><h3 id="在每一次进行梯度下降更新theta值时，只使用单个样本进行计算优点就是提高t训练速度，相反"><a href="#在每一次进行梯度下降更新theta值时，只使用单个样本进行计算优点就是提高t训练速度，相反" class="headerlink" title="　　在每一次进行梯度下降更新theta值时，只使用单个样本进行计算优点就是提高t训练速度，相反"></a>　　在每一次进行梯度下降更新theta值时，只使用单个样本进行计算优点就是提高t训练速度，相反</h3><h3 id="的这种方式就没有BGD得到的结果的准确性高。"><a href="#的这种方式就没有BGD得到的结果的准确性高。" class="headerlink" title="的这种方式就没有BGD得到的结果的准确性高。"></a>的这种方式就没有BGD得到的结果的准确性高。</h3><h4 id="————————————————————————————————————————————————————-1"><a href="#————————————————————————————————————————————————————-1" class="headerlink" title="————————————————————————————————————————————————————-"></a>————————————————————————————————————————————————————-</h4><h2 id="【MBGD–小批量梯度下降】"><a href="#【MBGD–小批量梯度下降】" class="headerlink" title="【MBGD–小批量梯度下降】"></a>【MBGD–小批量梯度下降】</h2><h3 id="这种方法算是为了同时考虑到速度和准确性在。每一次进行梯度下降更新theta值时，使用部分"><a href="#这种方法算是为了同时考虑到速度和准确性在。每一次进行梯度下降更新theta值时，使用部分" class="headerlink" title="　　这种方法算是为了同时考虑到速度和准确性在。每一次进行梯度下降更新theta值时，使用部分"></a>　　这种方法算是为了同时考虑到速度和准确性在。每一次进行梯度下降更新theta值时，使用部分</h3><h3 id="样本数据进行计算性高。"><a href="#样本数据进行计算性高。" class="headerlink" title="样本数据进行计算性高。"></a>样本数据进行计算性高。</h3><h4 id="————————————————————————————————————————————————————-2"><a href="#————————————————————————————————————————————————————-2" class="headerlink" title="————————————————————————————————————————————————————-"></a>————————————————————————————————————————————————————-</h4><h1 id="课后作业·劝退就得靠实战演练"><a href="#课后作业·劝退就得靠实战演练" class="headerlink" title="课后作业·劝退就得靠实战演练"></a>课后作业·劝退就得靠实战演练</h1><h2 id="题目：根据城市人口数量，预测开小吃店的利润-数据在ex1data1-txt里，第一列是城市人口数量，第"><a href="#题目：根据城市人口数量，预测开小吃店的利润-数据在ex1data1-txt里，第一列是城市人口数量，第" class="headerlink" title="题目：根据城市人口数量，预测开小吃店的利润 数据在ex1data1.txt里，第一列是城市人口数量，第"></a>题目：根据城市人口数量，预测开小吃店的利润 数据在ex1data1.txt里，第一列是城市人口数量，第</h2><h3 id="二列是该城市小吃店利润。"><a href="#二列是该城市小吃店利润。" class="headerlink" title="二列是该城市小吃店利润。"></a>二列是该城市小吃店利润。</h3><h3 id="我的答案：-注：我的答案和题目来源的作者有偏差，我不知道哪儿错了-另-文章结尾附上题目来源"><a href="#我的答案：-注：我的答案和题目来源的作者有偏差，我不知道哪儿错了-另-文章结尾附上题目来源" class="headerlink" title="我的答案：(注：我的答案和题目来源的作者有偏差，我不知道哪儿错了)另:文章结尾附上题目来源"></a>我的答案：(注：我的答案和题目来源的作者有偏差，我不知道哪儿错了)另:文章结尾附上题目来源</h3><h3 id="链接【白嫖真香-下次一定警告-老铁点个赞-lt-lt-】"><a href="#链接【白嫖真香-下次一定警告-老铁点个赞-lt-lt-】" class="headerlink" title="链接【白嫖真香,下次一定警告,老铁点个赞&lt;_&lt;】"></a>链接【白嫖真香,下次一定警告,老铁点个赞&lt;_&lt;】</h3><p> #<img src="/.io//%E6%A0%B8%E5%BF%83%E5%85%AC%E5%BC%8F.png" alt><br> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(X,y,theta)</span>:</span></span><br><span class="line">    cost_mar = np.power(((X*theta.T)-y),<span class="number">2</span>)</span><br><span class="line">    cost_sum = np.sum(cost_mar)/(<span class="number">2</span>*len(X))</span><br><span class="line">    <span class="keyword">return</span> cost_sum</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(X,y,theta,aph,time)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(time):</span><br><span class="line">        temp1 = theta</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line">            temp = X[:,j]</span><br><span class="line">            theta[<span class="number">0</span>,j]=theta[<span class="number">0</span>,j]-aph/len(X)*(np.sum((X*temp1.T-y).T*temp))</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">path = <span class="string">'C:/Users/Lenovo/Desktop/MLdata/ex1/ex1data1.txt'</span></span><br><span class="line">data = pd.read_csv(path,header=<span class="literal">None</span>,names=[<span class="string">'population'</span>,<span class="string">'profit'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data.insert(<span class="number">0</span>,<span class="string">'ones'</span>,<span class="number">1</span>)</span><br><span class="line">col = data.shape[<span class="number">1</span>]</span><br><span class="line">X = data.iloc[:,:<span class="number">-1</span>]</span><br><span class="line">y = data.iloc[:,col<span class="number">-1</span>:col]</span><br><span class="line">theta = np.array([[<span class="number">0.0</span>,<span class="number">0.0</span>]])</span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">y = np.matrix(y.values)</span><br><span class="line">theta = np.matrix(theta)</span><br><span class="line">aph = <span class="number">0.003</span></span><br><span class="line">time = <span class="number">10000</span></span><br><span class="line"><span class="comment">#print(X)</span></span><br><span class="line">theta = gradient(X,y,theta,aph,time)</span><br><span class="line">print(<span class="string">'theta:'</span>,theta)<span class="comment">#训练结果得到的theta向量</span></span><br><span class="line">predict1 = [<span class="number">1</span>,<span class="number">3.5</span>]*(theta).T</span><br><span class="line">print(<span class="string">'predict1:'</span>,predict1)</span><br><span class="line">predict2 = [<span class="number">1</span>,<span class="number">7</span>]*(theta).T</span><br><span class="line">print(<span class="string">'predict2:'</span>,predict2)</span><br><span class="line"></span><br><span class="line">x = np.linspace(data.population.min(), data.population.max(), <span class="number">100</span>)</span><br><span class="line">f = theta[<span class="number">0</span>, <span class="number">0</span>] + (theta[<span class="number">0</span>, <span class="number">1</span>] * x)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.plot(x, f, <span class="string">'r'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">ax.scatter(data.population, data.profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">ax.legend(loc=<span class="number">2</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Predicted Profit for Population Size'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<h2 id="训练结果-采用BGD下降算法"><a href="#训练结果-采用BGD下降算法" class="headerlink" title="训练结果(采用BGD下降算法)"></a>训练结果(采用BGD下降算法)</h2><h3 id="这里我将theta初始值假设为-0-0-aph为0-03，迭代更新次数为10000"><a href="#这里我将theta初始值假设为-0-0-aph为0-03，迭代更新次数为10000" class="headerlink" title="这里我将theta初始值假设为[0,0],aph为0.03，迭代更新次数为10000"></a>这里我将theta初始值假设为[0,0],aph为0.03，迭代更新次数为10000</h3><p><img src="/.io//result.png" alt><br><img src="/.io//chart1.png" alt></p>
<h4 id="———假——-装——-我——-是——-一——-条——-分——-割——-线——–-1"><a href="#———假——-装——-我——-是——-一——-条——-分——-割——-线——–-1" class="headerlink" title="———假——-装——-我——-是——-一——-条——-分——-割——-线——–"></a>———假——-装——-我——-是——-一——-条——-分——-割——-线——–</h4><h3 id="老铁们，看到错误一定要在评论里告诉我，不告诉我的都是我搬砖路上的绊脚石。最后附上题"><a href="#老铁们，看到错误一定要在评论里告诉我，不告诉我的都是我搬砖路上的绊脚石。最后附上题" class="headerlink" title="　　老铁们，看到错误一定要在评论里告诉我，不告诉我的都是我搬砖路上的绊脚石。最后附上题"></a>　　老铁们，看到错误一定要在评论里告诉我，不告诉我的都是我搬砖路上的绊脚石。最后附上题</h3><h3 id="目资源链接。谢谢大家-相互学习-共同进步-一起搬砖"><a href="#目资源链接。谢谢大家-相互学习-共同进步-一起搬砖" class="headerlink" title="目资源链接。谢谢大家!相互学习!共同进步!一起搬砖!"></a>目资源链接。谢谢大家!相互学习!共同进步!一起搬砖!</h3><h3 id="下面给出让人血脉喷张-内容不可描述的网址-一定要自己晚上偷偷看哦-滚吧【狗头】："><a href="#下面给出让人血脉喷张-内容不可描述的网址-一定要自己晚上偷偷看哦-滚吧【狗头】：" class="headerlink" title="下面给出让人血脉喷张,内容不可描述的网址,一定要自己晚上偷偷看哦,滚吧【狗头】："></a>下面给出让人血脉喷张,内容不可描述的网址,一定要自己晚上偷偷看哦,滚吧【狗头】：</h3><h3 id="https-www-kesci-com-home-project-5da16a37037db3002d441810"><a href="#https-www-kesci-com-home-project-5da16a37037db3002d441810" class="headerlink" title="https://www.kesci.com/home/project/5da16a37037db3002d441810"></a><a href="https://www.kesci.com/home/project/5da16a37037db3002d441810" target="_blank" rel="noopener">https://www.kesci.com/home/project/5da16a37037db3002d441810</a></h3>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>About  Me</title>
    <url>/2020/03/27/About/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>姓名：张玉想<br>性别：男<br>现居地：江西南昌<br>职业：东华理工大学 | 计算机科学与技术 | 本科<br>爱好：人工智能 | 计算机视觉 | 摄影 | 旅行<br>github: <a href="https://github.com/zyx-cv" target="_blank" rel="noopener">https://github.com/zyx-cv</a><br>QQ：1300281401(非诚勿扰)<br>哔哩哔哩：Shutterbug张<br>图虫：Shutterbug张<br>CSDN：小张小张,考试不慌</p>
]]></content>
  </entry>
</search>
